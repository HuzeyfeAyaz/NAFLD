{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.patches as mpatches\n",
    "import sys\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from impyute.imputation.cs import mice\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.multiclass import OneVsRestClassifier as ovr\n",
    "\n",
    "# Tree Visualization\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Scores\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Ensemble models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignoring Errors\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_val_score(classifier, X, y):\n",
    "#     scores = cross_val_score(estimator=classifier,\n",
    "#                          X=X,\n",
    "#                          y=y,\n",
    "#                          cv=10,\n",
    "#                          scoring='f1_sample')\n",
    "#     return (scores.mean(), scores.std())\n",
    "scorer = make_scorer(f1_score, pos_label = None, average = 'micro')\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "def find_best(classifier, X_train, y_train, param_grid, scoring=scorer):\n",
    "    gs = GridSearchCV(estimator=classifier, \n",
    "                      param_grid=param_grid, \n",
    "                      scoring=scoring,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1)\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    return (gs.best_score_, gs.best_params_, gs.best_estimator_)\n",
    "\n",
    "# Finding roc_curve and Auc score for each classifier\n",
    "def find_roccurve(classifier,X_train,X_test,y_train,y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_prob = classifier.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    # print('AUC: %.2f' % auc)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    return (auc, fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_score(classifier, X, y):\n",
    "    all_f1_scores = []\n",
    "#     kf = KFold(10, random_state=0)\n",
    "    kf = StratifiedKFold(10, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "#         f1_balance = (f1_score(y_test, y_pred, pos_label=0)+f1_score(y_test, y_pred, pos_label=1))/2\n",
    "        all_f1_scores.append(f1_score(y_test, y_pred, pos_label=None, average='micro'))\n",
    "    \n",
    "    all_f1_scores = np.ravel(all_f1_scores)\n",
    "    return (all_f1_scores.mean(), all_f1_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_f1_scores(X, y, X_train, y_train):\n",
    "    pipe_lr = Pipeline([['sc', StandardScaler()], ['clf', ovr(LogisticRegression(random_state=0))]])\n",
    "    pipe_svm = Pipeline([['sc', StandardScaler()], ['clf', ovr(SVC(probability=True))]])\n",
    "    pipe_knn = Pipeline([['sc', StandardScaler()], ['clf', ovr(KNeighborsClassifier(n_jobs=-1))]])\n",
    "    pipe_dt = Pipeline([['sc', StandardScaler()], ['clf', ovr(DecisionTreeClassifier(random_state=0))]])\n",
    "\n",
    "    lr_grid = [{'clf__estimator__C': param_range,\n",
    "                'clf__estimator__penalty': ['l1','l2']}]\n",
    "\n",
    "    svm_grid = [{'clf__estimator__C': param_range,\n",
    "                'clf__estimator__kernel': ['rbf','sigmoid']}]\n",
    "    \n",
    "    knn_grid = [{'clf__estimator__n_neighbors': [5, 10, 30, 50]}]\n",
    "    \n",
    "    dt_grid = [{'clf__estimator__max_depth': [None, 1, 2, 3]}]\n",
    "\n",
    "    lr_params = find_best(pipe_lr, X_train, y_train, lr_grid)\n",
    "    svm_params = find_best(pipe_svm, X_train, y_train, svm_grid)\n",
    "    knn_params = find_best(pipe_knn, X_train, y_train, knn_grid)\n",
    "    dt_params = find_best(pipe_dt, X_train, y_train, dt_grid)\n",
    "\n",
    "    lr_score = find_val_score(lr_params[2], X, y)\n",
    "    svm_score = find_val_score(svm_params[2], X, y)\n",
    "    knn_score = find_val_score(knn_params[2], X, y)\n",
    "    dt_score = find_val_score(dt_params[2], X, y)\n",
    "\n",
    "    return (lr_score, svm_score, knn_score, dt_score, (lr_params[2], svm_params[2], knn_params[2], dt_params[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_f1_scores(num, save=False):\n",
    "    scores = baseline_f1_scores[num]['score']\n",
    "    name = baseline_f1_scores[num]['name']\n",
    "\n",
    "    scores = {'score':[scores[0], scores[1], scores[2], scores[3]]}\n",
    "\n",
    "    class_names = {'score':'Baseline Approach F1-Scores'}\n",
    "\n",
    "    classifier_names = ['LR', 'SVM', 'KNN', 'DT']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "    clr = ('orange','blue', 'green', 'purple', 'red', 'purple')\n",
    "    for idx, mthod in zip([i for i in range(1)], scores.keys()):\n",
    "        ax.bar(classifier_names, [round(i[0]*100,2) for i in scores[mthod]], \n",
    "                               align='center', alpha=1.0, color=clr)\n",
    "        ax.set_yticks([i for i in range(0,110,10)])\n",
    "        legends = []\n",
    "        for i in range(len(classifier_names)):\n",
    "            legends.append(mpatches.Patch(color=clr[i], label='{}: {}'.format(classifier_names[i], \n",
    "                                                                              round(scores[mthod][i][0]*100,2))))\n",
    "        ax.legend(handles=legends,loc='best')\n",
    "        ax.set_xlabel('Classifiers')\n",
    "        ax.set_ylabel('F1 Scores')\n",
    "        ax.title.set_text(class_names[mthod])\n",
    "    # fig.tight_layout()\n",
    "    fig.suptitle('Baseline Approach F1-Scores on\\n\\\"{}\\\" column'.format(name),\n",
    "                 y=1.03, fontsize=14)\n",
    "    if save:\n",
    "        # 'f1_scores_knn_mice'\n",
    "        fig.savefig(save, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_roc_curves(classifiers, X_train, X_test, y_train, y_test):\n",
    "    lr_score = find_roccurve(classifiers[0], X_train, X_test, y_train, y_test)\n",
    "    svm_score = find_roccurve(classifiers[1], X_train, X_test, y_train, y_test)\n",
    "    knn_score = find_roccurve(classifiers[2], X_train, X_test, y_train, y_test)\n",
    "    dt_score = find_roccurve(classifiers[3], X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    return (lr_score, svm_score, knn_score, dt_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(roc_scores, num, save=False):\n",
    "    scores = roc_scores[num]['score']\n",
    "    name_ = roc_scores[num]['name']\n",
    "\n",
    "    scores = {'score':[scores[0], scores[1], scores[2], scores[3]]}\n",
    "    \n",
    "    classifier_names = ['LR', 'SVM', 'KNN', 'DT']\n",
    "    clr = ('orange','blue', 'green', 'purple', 'red', 'purple')\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "\n",
    "    for idx, key in zip([0,1], scores.keys()):\n",
    "        data = scores[key]\n",
    "        for name, clor, score_idx in zip(classifier_names, clr[1:5], range(4)):\n",
    "            inf = data[score_idx]\n",
    "            ax.plot(inf[1], inf[2], color=clor, label='{}: {}'.format(name, round(inf[0],2)))\n",
    "        ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        \n",
    "    fig.suptitle('Roc Curves of Baseline (15 Features) Approach on\\n\\\"{}\\\" column'.format(name_),\n",
    "                 y=1.02, fontsize=14)\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_scores(X_train, X_test, y_train, y_test, regressor):\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    mae_score = mean_absolute_error(y_test, y_pred)\n",
    "    mse_score = mean_squared_error(y_test, y_pred)\n",
    "    r2_score_ = r2_score(y_test, y_pred)\n",
    "    return (mae_score, mse_score, r2_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_error_scores(X_train, X_test, y_train, y_test):\n",
    "    pipe_lr_reg = Pipeline([['sc', StandardScaler()], ['clf', LinearRegression(n_jobs=-1)]])\n",
    "    pipe_svr_reg = Pipeline([['sc', StandardScaler()], ['clf', SVR()]])\n",
    "    pipe_knn_reg = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsRegressor(n_jobs=-1)]])\n",
    "    pipe_dt_reg = Pipeline([['sc', StandardScaler()], ['clf', DecisionTreeRegressor(random_state=0)]])\n",
    "\n",
    "    svm_grid = [{'clf__C': param_range,\n",
    "                'clf__kernel': ['rbf','sigmoid']}]\n",
    "    \n",
    "    knn_grid = [{'clf__n_neighbors': [5, 10, 30, 50]}]\n",
    "    \n",
    "    dt_grid = [{'clf__max_depth': [None, 1, 2, 3]}]\n",
    "\n",
    "    svr_params = find_best(pipe_svr_reg, X_train, y_train, svm_grid, scoring='neg_mean_absolute_error')\n",
    "    knn_params = find_best(pipe_knn_reg, X_train, y_train, knn_grid, scoring='neg_mean_absolute_error')\n",
    "    dt_params = find_best(pipe_dt_reg, X_train, y_train, dt_grid, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    lr_errors = calculate_error_scores(X_train, X_test, y_train, y_test, pipe_lr_reg)\n",
    "    svr_errors = calculate_error_scores(X_train, X_test, y_train, y_test, svr_params[2])\n",
    "    knn_errors = calculate_error_scores(X_train, X_test, y_train, y_test, knn_params[2])\n",
    "    dt_errors = calculate_error_scores(X_train, X_test, y_train, y_test, dt_params[2])\n",
    "\n",
    "    return (lr_errors, svr_errors, knn_errors, dt_errors, (pipe_lr_reg, svr_params[2], knn_params[2], dt_params[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_scores(error_scores, num, title, save=False):\n",
    "    scores = error_scores[num]['score']\n",
    "    name = error_scores[num]['name']\n",
    "    scores = {'lr':[scores['lr'][0], scores['lr'][1], scores['lr'][2]],\n",
    "             'svr':[scores['svr'][0], scores['svr'][1], scores['svr'][2]],\n",
    "             'knn':[scores['knn'][0], scores['knn'][1], scores['knn'][2]],\n",
    "             'dt':[scores['dt'][0], scores['dt'][1], scores['dt'][2]]}\n",
    "    \n",
    "    class_names = {'lr': 'LinearRegression', 'svr': 'SupportVector Regressor',\n",
    "                   'knn':'KNN Regressor', 'dt':'DecisionTree Regressor'}\n",
    "\n",
    "    scores_names = ['MAE', 'MSE', 'R2']\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,7))\n",
    "    ax = np.ravel(ax)\n",
    "    clr = ('orange','blue', 'green', 'purple', 'red', 'purple')\n",
    "    for idx, mthod in zip([i for i in range(4)], scores.keys()):\n",
    "        ax[idx].bar(scores_names, [round(i,3) for i in scores[mthod]], \n",
    "                               align='center', alpha=1.0, color=clr)\n",
    "        ax[idx].set_yticks([a for a in range(0,4)])\n",
    "        legends = []\n",
    "        for c in range(len(scores_names)):\n",
    "            legends.append(mpatches.Patch(color=clr[c], label='{}: {}'.format(scores_names[c], \n",
    "                                                                              round(scores[mthod][c],3))))\n",
    "        ax[idx].legend(handles=legends,loc='best')\n",
    "        ax[idx].set_xlabel('Error Algorithms')\n",
    "        ax[idx].set_ylabel(\"Error Algorithms' Scores\")\n",
    "        ax[idx].title.set_text('MAE, MSE & R2 Scores By {}'.format(class_names[mthod]))\n",
    "    fig.suptitle('Error Scores of {} For \\\"{}\\\" column'.format(title, name),y=1.02, fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(save, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_confusion(classifiers, X_train, X_test, y_train, y_test):\n",
    "    confmats = []\n",
    "    for classifier in classifiers:\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "        confmats.append(confmat)\n",
    "    return confmats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(confmats, num, save=False):\n",
    "    scores = confmats[num]['score']\n",
    "    name = confmats[num]['name']\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    for ax_num, classifier in zip([221,222,223,224], scores.keys()):\n",
    "        ax = plt.subplot(ax_num)\n",
    "        ax.title.set_text(classifier)\n",
    "        confmat = scores[classifier]\n",
    "\n",
    "        ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "        for i in range(confmat.shape[0]):\n",
    "            for j in range(confmat.shape[1]):\n",
    "                ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "        \n",
    "        plt.xlabel('predicted label')\n",
    "        plt.ylabel('true label')\n",
    "\n",
    "    plt.suptitle('Confusion Matrix of Baseline (15 Features) Approach on\\n\\\"{}\\\" column'.format(name), y=1.05, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(save, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_visualization(num, dt, features, target, save_name):\n",
    "    columns = features.columns\n",
    "    dt.fit(features.values, target)\n",
    "\n",
    "    export_graphviz(dt['clf'], out_file=save_name, feature_names=columns, filled=True, rounded=True, \n",
    "                    special_characters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"NAFLD_en.xlsx\")\n",
    "features_df = data.iloc[:,1:-9]\n",
    "targets_df = data.iloc[:,-9:]\n",
    "\n",
    "combined_fibrosis = targets_df.iloc[:,3].values\n",
    "counter = 2\n",
    "for trgt in targets_df.iloc[:,4:7].columns:\n",
    "    for idx in range(targets_df[trgt].values.shape[0]):\n",
    "        if targets_df[trgt].values[idx] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            combined_fibrosis[idx] = counter\n",
    "    counter+=1\n",
    "    \n",
    "# lb = LabelBinarizer()\n",
    "# bnd_labels = lb.fit_transform(combined_fibrosis)\n",
    "new_targets_df = targets_df.drop(list(targets_df.iloc[:, 3:7].columns),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm in targets_df.columns[-2:]:\n",
    "    targets_df[clm] = targets_df[clm].map({1:0, 2:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_counts = {}\n",
    "for col in features_df.columns:\n",
    "    missing_val_counts[col] = features_df[col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = []\n",
    "for col, value in missing_val_counts.items():\n",
    "    if value == 0:\n",
    "        baseline_features.append(col)\n",
    "baseline_features = features_df[baseline_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_baseline = baseline_features.drop(['Steatosis', 'Activity'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dropped_baseline.values, combined_fibrosis,\n",
    "                                                                test_size = 0.3, random_state = 0, stratify=combined_fibrosis)\n",
    "\n",
    "f1_scores = return_f1_scores(dropped_baseline.values, combined_fibrosis, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.3649612937939893, 0.05578263405145976),\n",
       " (0.37710961840540846, 0.03805446396650743),\n",
       " (0.38427117776215647, 0.05106353901121417),\n",
       " (0.36270383677436707, 0.044026929724593565))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_f1_scores(0, save=\"baseline_avarage_f1_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_f1_scores(1, save=\"baseline_avarage_f1_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
