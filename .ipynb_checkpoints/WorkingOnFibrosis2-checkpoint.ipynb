{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Tables By Metrice Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from impyute.imputation.cs import fast_knn\n",
    "import pickle\n",
    "\n",
    "# Scores\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Ignoring Errors\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "scoring = make_scorer(roc_auc_score)\n",
    "def gsearch(classifier, X_train, y_train, param_grid, scoring=scoring):\n",
    "    gs = GridSearchCV(estimator=classifier,\n",
    "                      param_grid=param_grid, \n",
    "                      scoring=scoring,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1)\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    return (gs.best_score_, gs.best_params_, gs.best_estimator_)\n",
    "\n",
    "def collect_scores(classifier, X_train, X_test, y_train, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_prob = classifier.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "    \n",
    "    tpr = tp/(tp + fn)\n",
    "    tnr = tn/(fp + tn)\n",
    "    ppv = tp/(tp + fp)\n",
    "    npv = tn/(tn + fn)\n",
    "    plr = tpr/(1 - tnr)\n",
    "    nlr = (1 - tpr)/tnr\n",
    "    y_test_size = y_test.shape[0]\n",
    "    fn = fn/y_test_size\n",
    "    fp = fp/y_test_size\n",
    "    return [tpr, tnr, fn, fp, ppv, npv, plr, nlr, auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_estimator(X, y):\n",
    "    best_lr, best_svm, best_rf = [0, 0], [0, 0], [0, 0]\n",
    "\n",
    "    pipe_lr = Pipeline([['sc', StandardScaler()], ['clf', LogisticRegression(random_state=1)]])\n",
    "    pipe_svm = Pipeline([['sc', StandardScaler()], ['clf', SVC(probability=True, random_state=1)]])\n",
    "    rf = RandomForestClassifier(random_state=1, n_jobs=-1)\n",
    "\n",
    "    lr_grid = [{'clf__C': param_range,\n",
    "            'clf__penalty': ['l1','l2']}]\n",
    "\n",
    "    svm_grid = [{'clf__C': param_range,\n",
    "                'clf__kernel': ['rbf','sigmoid']}]\n",
    "    \n",
    "    rf_grid = [{\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]}]\n",
    "\n",
    "    kf = StratifiedKFold(10, random_state=1)\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        lr = gsearch(pipe_lr, X_train, y_train, lr_grid)\n",
    "        svm = gsearch(pipe_svm, X_train, y_train, svm_grid)\n",
    "        rf_best = gsearch(rf, X_train, y_train, rf_grid)\n",
    "        if lr[0] > best_lr[0]:\n",
    "            best_lr[0] = lr[0]\n",
    "            best_lr[1] = lr\n",
    "        if svm[0] > best_svm[0]: \n",
    "            best_svm[0] = svm[0]\n",
    "            best_svm[1] = svm\n",
    "        if rf_best[0] > best_rf[0]: \n",
    "            best_rf[0] = rf_best[0]\n",
    "            best_rf[1] = rf_best\n",
    "            \n",
    "    return (best_lr[1][2], best_svm[1][2], best_rf[1][2], (best_lr[1][1], best_svm[1][1], best_rf[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_confusion(classifiers, X_train, X_test, y_train, y_test):\n",
    "    confmats = []\n",
    "    for classifier in classifiers:\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "        confmats.append(confmat)\n",
    "    return confmats\n",
    "\n",
    "def plot_confusion(classifiers, X_train, X_test, y_train, y_test, title, save=False):\n",
    "    clf_names = [\"LR\", \"SVM\", \"RF\", \"BAGGING\"]\n",
    "    confmats = return_confusion(classifiers, X_train, X_test, y_train, y_test)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for ax_num, cnfm, clf_name in zip([221,222,223,224], confmats, clf_names):\n",
    "        ax = plt.subplot(ax_num)\n",
    "        ax.title.set_text(clf_name)\n",
    "\n",
    "        ax.matshow(cnfm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "        for i in range(cnfm.shape[0]):\n",
    "            for j in range(cnfm.shape[1]):\n",
    "                ax.text(x=j, y=i, s=cnfm[i, j], va='center', ha='center')\n",
    "        \n",
    "        plt.xlabel('predicted label')\n",
    "        plt.ylabel('true label')\n",
    "\n",
    "    plt.suptitle(title, y=1.05, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(save, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"NAFLD with FIB4 and NFS Scores.xlsx\").drop(294)\n",
    "features_df = pd.concat([data.iloc[:,1:-13], data[['FIB-4', 'NFS']]], axis=1)\n",
    "targets_df = data[['Significant Fibrosis (No=0, Yes=1) (If Fibrosis 2 and above, there is Significant Fibrosis)',\n",
    "                         'Advanced Fibrosis (No=0, Yes=1) (If Fibrosis is 3 and above, there is Advanced Fibrosis)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_counts = dict(features_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2 = features_df.dropna(axis=1) # shape: (604, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (No missed values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_4_SF = best_estimator(baseline2.values, targets_df.iloc[:,0].values)\n",
    "models_4_AF = best_estimator(baseline2.values, targets_df.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('best_models.db', 'rb') as models:\n",
    "#     keeping_models = pickle.load(models)\n",
    "    \n",
    "#     models_4_SF = keeping_models['0']['SF']\n",
    "#     models_4_AF = keeping_models['0']['AF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_4_AF[3] # best parameters of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_sf, svm_best_sf, rf_best_sf = models_4_SF[0], models_4_SF[1], models_4_SF[2]\n",
    "lr_best_af, svm_best_af, rf_best_af = models_4_AF[0], models_4_AF[1], models_4_AF[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(DecisionTreeClassifier(), n_jobs=-1, random_state=1)\n",
    "\n",
    "sf_models = [lr_best_sf, svm_best_sf, rf_best_sf, bagging]\n",
    "af_models = [lr_best_af, svm_best_af, rf_best_af, bagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = {}\n",
    "kf = StratifiedKFold(10, random_state=1)\n",
    "X = baseline2.values\n",
    "\n",
    "for idx, models in enumerate([sf_models, af_models]):\n",
    "    if idx == 0: target = \"SF\"\n",
    "    else: target = \"AF\"\n",
    "    comparison_table.setdefault(target, [])\n",
    "    for mdl in models:\n",
    "        avrg = []\n",
    "        y = targets_df.iloc[:,idx].values\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            avrg.append(collect_scores(mdl, X_train, X_test, y_train, y_test))\n",
    "        avrg = pd.DataFrame(np.array(avrg)).replace({np.inf:np.nan}).values\n",
    "        avrg_list = np.round(np.nanmean(avrg, axis=0, dtype=np.float64),3).tolist()\n",
    "        avrg_list[0], avrg_list[1] = round(avrg_list[0]*100), round(avrg_list[1]*100)\n",
    "        avrg_list[2], avrg_list[3] = avrg_list[2], avrg_list[3]\n",
    "        comparison_table[target].append(avrg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Sensitivity (%)', 'Specificity (%)', 'FN', 'FP', 'PPV', 'NPV', 'PLR', 'NLR', 'AUC']\n",
    "index = ['LR', 'SVM', 'RandomForest', 'Bagging']\n",
    "index1 = pd.MultiIndex.from_tuples([('SIGNIFICANT', 'LR'), ('FIBROSIS', 'SVM'), ('','RandomForest'), ('', 'Bagging')])\n",
    "sf_table_df = pd.DataFrame(np.array(comparison_table['SF']), index=index1, columns=columns)\n",
    "sf_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index2 = pd.MultiIndex.from_tuples([('ADVANCED', 'LR'), ('FIBROSIS', 'SVM'), ('', 'RandomForest'), ('', 'Bagging')])\n",
    "af_table_df = pd.DataFrame(np.array(comparison_table['AF']), index=index2, columns=columns)\n",
    "af_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf_0, X_test_sf_0, y_train_sf_0, y_test_sf_0 = train_test_split(\n",
    "    baseline2.values, targets_df.iloc[:,0].values, test_size=0.2, random_state=1, stratify= y)\n",
    "\n",
    "X_train_af_0, X_test_af_0, y_train_af_0, y_test_af_0 = train_test_split(\n",
    "    baseline2.values, targets_df.iloc[:,1].values, test_size=0.2, random_state=1, stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_title_0 = \"Significant Fibrosis Without Missed Data\"\n",
    "plot_confusion(sf_models, X_train_sf_0, X_test_sf_0, y_train_sf_0, y_test_sf_0, sf_title_0, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_title_0 = \"Advanced Fibrosis Without Missed Data\"\n",
    "plot_confusion(af_models, X_train_af_0, X_test_af_0, y_train_af_0, y_test_af_0, af_title_0, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_percentages = features_df.isnull().mean()\n",
    "threshold_25_df = features_df.iloc[:, missing_data_percentages.values < 0.25]\n",
    "knn_25 = fast_knn(threshold_25_df.values, k=30) # shape: (604, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_25_SF = best_estimator(knn_25, targets_df.iloc[:,0].values)\n",
    "models_25_AF = best_estimator(knn_25, targets_df.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('best_models.db', 'rb') as models:\n",
    "#     keeping_models = pickle.load(models)\n",
    "    \n",
    "#     models_25_SF = keeping_models['25']['SF']\n",
    "#     models_25_AF = keeping_models['25']['AF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_25_AF[3] # best parameters of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_sf_25, svm_best_sf_25, rf_best_sf_25 = models_25_SF[0], models_25_SF[1], models_25_SF[2]\n",
    "lr_best_af_25, svm_best_af_25, rf_best_af_25 = models_25_AF[0], models_25_AF[1], models_25_AF[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(DecisionTreeClassifier(), n_jobs=-1, random_state=1)\n",
    "\n",
    "sf_25_models = [lr_best_sf_25, svm_best_sf_25, rf_best_sf_25, bagging]\n",
    "af_25_models = [lr_best_af_25, svm_best_af_25, rf_best_af_25, bagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparison_table_25 = {}\n",
    "kf = StratifiedKFold(10, random_state=1)\n",
    "X = knn_25\n",
    "\n",
    "for idx, models in enumerate([sf_25_models, af_25_models]):\n",
    "    if idx == 0: target = \"SF\"\n",
    "    else: target = \"AF\"\n",
    "    comparison_table_25.setdefault(target, [])\n",
    "    for mdl in models:\n",
    "        avrg = []\n",
    "        y = targets_df.iloc[:,idx].values\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            avrg.append(collect_scores(mdl, X_train, X_test, y_train, y_test))\n",
    "        avrg = pd.DataFrame(np.array(avrg)).replace({np.inf:np.nan}).values\n",
    "        avrg_list = np.round(np.nanmean(avrg, axis=0, dtype=np.float64),3).tolist()\n",
    "        avrg_list[0], avrg_list[1] = round(avrg_list[0]*100), round(avrg_list[1]*100)\n",
    "        avrg_list[2], avrg_list[3] = avrg_list[2], avrg_list[3]\n",
    "        comparison_table_25[target].append(avrg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index3 = pd.MultiIndex.from_tuples([('SIGNIFICANT', 'LR'), ('FIBROSIS', 'SVM'), ('', 'RandomForest'), ('', 'Bagging')])\n",
    "sf_table_25_df = pd.DataFrame(np.array(comparison_table_25['SF']), index=index3, columns=columns)\n",
    "sf_table_25_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index4 = pd.MultiIndex.from_tuples([('ADVANCED', 'LR'), ('FIBROSIS', 'SVM'), ('', 'RandomForest'), ('', 'Bagging')])\n",
    "af_table_25_df = pd.DataFrame(np.array(comparison_table_25['AF']), index=index4, columns=columns)\n",
    "af_table_25_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sf_25, X_test_sf_25, y_train_sf_25, y_test_sf_25 = train_test_split(\n",
    "    knn_25, targets_df.iloc[:,0].values, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "X_train_af_25, X_test_af_25, y_train_af_25, y_test_af_25 = train_test_split(\n",
    "    knn_25, targets_df.iloc[:,1].values, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_title_25 = \"Significant Fibrosis With 25% Threshold\"\n",
    "plot_confusion(sf_25_models, X_train_sf_25, X_test_sf_25, y_train_sf_25, y_test_sf_25, sf_title_25, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_title_25 = \"Advanced Fibrosis With 25% Threshold\"\n",
    "plot_confusion(af_25_models, X_train_af_25, X_test_af_25, y_train_af_25, y_test_af_25, af_title_25, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Models In The Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_models = {'0':{'SF':models_4_SF, 'AF':models_4_AF},\n",
    "               '25':{'SF':models_25_SF, 'AF':models_25_AF}}\n",
    "with open('best_models_with_additional_features.db','wb') as models:\n",
    "    pickle.dump(keep_models, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models.db', 'rb') as models:\n",
    "#     keeping_models = pickle.load(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import table\n",
    "\n",
    "# tables = [sf_table_df, af_table_df, sf_table_25_df, af_table_25_df]\n",
    "# table_names = ['sf_table.png', 'af_table.png', 'sf_25_table.png', 'af_25_table.png']\n",
    "# for tbl, name in zip(tables, table_names):\n",
    "#     ax = plt.subplot(111, frame_on=False) # no visible frame\n",
    "#     ax.xaxis.set_visible(False)  # hide the x axis\n",
    "#     ax.yaxis.set_visible(False)  # hide the y axis\n",
    "\n",
    "#     table(ax, tbl)\n",
    "#     plt.savefig(name, dpi=600, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
